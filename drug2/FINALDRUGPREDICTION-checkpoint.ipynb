{"cells":[{"cell_type":"code","execution_count":1,"id":"5a0fe18d","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":976,"status":"ok","timestamp":1682517136918,"user":{"displayName":"Asif Nawaz","userId":"01704054617184056097"},"user_tz":-300},"id":"5a0fe18d","outputId":"bfb49641-fa72-45f9-c275-e2ca48765474"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Error loading stopwords: <urlopen error [WinError 10060] A\n","[nltk_data]     connection attempt failed because the connected party\n","[nltk_data]     did not properly respond after a period of time, or\n","[nltk_data]     established connection failed because connected host\n","[nltk_data]     has failed to respond>\n","[nltk_data] Error loading wordnet: <urlopen error [WinError 10060] A\n","[nltk_data]     connection attempt failed because the connected party\n","[nltk_data]     did not properly respond after a period of time, or\n","[nltk_data]     established connection failed because connected host\n","[nltk_data]     has failed to respond>\n","[nltk_data] Error loading punkt: <urlopen error [WinError 10060] A\n","[nltk_data]     connection attempt failed because the connected party\n","[nltk_data]     did not properly respond after a period of time, or\n","[nltk_data]     established connection failed because connected host\n","[nltk_data]     has failed to respond>\n","[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error\n","[nltk_data]     [WinError 10060] A connection attempt failed because\n","[nltk_data]     the connected party did not properly respond after a\n","[nltk_data]     period of time, or established connection failed\n","[nltk_data]     because connected host has failed to respond>\n"]},{"ename":"LookupError","evalue":"\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\adnan/nltk_data'\n    - 'c:\\\\Users\\\\adnan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\nltk_data'\n    - 'c:\\\\Users\\\\adnan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\adnan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\adnan\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)","File \u001b[1;32mc:\\Users\\adnan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mfind(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubdir\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00mzip_name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     85\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m:\n","File \u001b[1;32mc:\\Users\\adnan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n","\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\adnan/nltk_data'\n    - 'c:\\\\Users\\\\adnan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\nltk_data'\n    - 'c:\\\\Users\\\\adnan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\adnan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\adnan\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n","\nDuring handling of the above exception, another exception occurred:\n","\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)","\u001b[1;32md:\\all F proj\\APP\\FINALDRUGPREDICTION-checkpoint.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 42>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/all%20F%20proj/APP/FINALDRUGPREDICTION-checkpoint.ipynb#W0sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m vectorizer, classifier   \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/all%20F%20proj/APP/FINALDRUGPREDICTION-checkpoint.ipynb#W0sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/all%20F%20proj/APP/FINALDRUGPREDICTION-checkpoint.ipynb#W0sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m vectorizer, classifier \u001b[39m=\u001b[39m train_model(data, labels)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/all%20F%20proj/APP/FINALDRUGPREDICTION-checkpoint.ipynb#W0sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39m# Prediction function\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/all%20F%20proj/APP/FINALDRUGPREDICTION-checkpoint.ipynb#W0sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict_ingredients\u001b[39m(text, vectorizer, classifier, threshold\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/all%20F%20proj/APP/FINALDRUGPREDICTION-checkpoint.ipynb#W0sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     \u001b[39m# Preprocess the input text\u001b[39;00m\n","\u001b[1;32md:\\all F proj\\APP\\FINALDRUGPREDICTION-checkpoint.ipynb Cell 1\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(data, labels)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/all%20F%20proj/APP/FINALDRUGPREDICTION-checkpoint.ipynb#W0sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m vectorizer \u001b[39m=\u001b[39m CountVectorizer(preprocessor\u001b[39m=\u001b[39mpreprocess_text)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/all%20F%20proj/APP/FINALDRUGPREDICTION-checkpoint.ipynb#W0sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m# Convert the text data into a bag-of-words representation\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/all%20F%20proj/APP/FINALDRUGPREDICTION-checkpoint.ipynb#W0sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m data \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39;49mfit_transform(data[\u001b[39m'\u001b[39;49m\u001b[39mDrug Name\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mtolist())  \u001b[39m# Use the 'Drug Name' column as data\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/all%20F%20proj/APP/FINALDRUGPREDICTION-checkpoint.ipynb#W0sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39m# Train a Naive Bayes classifier on the data\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/all%20F%20proj/APP/FINALDRUGPREDICTION-checkpoint.ipynb#W0sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m classifier \u001b[39m=\u001b[39m MultinomialNB()\n","File \u001b[1;32mc:\\Users\\adnan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1330\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1322\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1323\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1324\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1325\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1326\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1327\u001b[0m             )\n\u001b[0;32m   1328\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1330\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[0;32m   1332\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   1333\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n","File \u001b[1;32mc:\\Users\\adnan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1201\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1199\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[0;32m   1200\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[1;32m-> 1201\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[0;32m   1202\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1203\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n","File \u001b[1;32mc:\\Users\\adnan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:113\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    112\u001b[0m     \u001b[39mif\u001b[39;00m preprocessor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 113\u001b[0m         doc \u001b[39m=\u001b[39m preprocessor(doc)\n\u001b[0;32m    114\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    115\u001b[0m         doc \u001b[39m=\u001b[39m tokenizer(doc)\n","\u001b[1;32md:\\all F proj\\APP\\FINALDRUGPREDICTION-checkpoint.ipynb Cell 1\u001b[0m in \u001b[0;36mpreprocess_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/all%20F%20proj/APP/FINALDRUGPREDICTION-checkpoint.ipynb#W0sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m tokens \u001b[39m=\u001b[39m [token\u001b[39m.\u001b[39mlower() \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m tokens]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/all%20F%20proj/APP/FINALDRUGPREDICTION-checkpoint.ipynb#W0sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Remove stop words\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/all%20F%20proj/APP/FINALDRUGPREDICTION-checkpoint.ipynb#W0sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m stop_words \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(stopwords\u001b[39m.\u001b[39;49mwords(\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/all%20F%20proj/APP/FINALDRUGPREDICTION-checkpoint.ipynb#W0sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m tokens \u001b[39m=\u001b[39m [token \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m tokens \u001b[39mif\u001b[39;00m token \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m stop_words]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/all%20F%20proj/APP/FINALDRUGPREDICTION-checkpoint.ipynb#W0sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# Join tokens back into a string\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\adnan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39mif\u001b[39;00m attr \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mLazyCorpusLoader object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__load()\n\u001b[0;32m    122\u001b[0m \u001b[39m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[39m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, attr)\n","File \u001b[1;32mc:\\Users\\adnan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfind(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubdir\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mzip_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[39m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__reader_cls(root, \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__kwargs)\n","File \u001b[1;32mc:\\Users\\adnan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mfind(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubdir\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     82\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[39mtry\u001b[39;00m:\n","File \u001b[1;32mc:\\Users\\adnan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n","\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\adnan/nltk_data'\n    - 'c:\\\\Users\\\\adnan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\nltk_data'\n    - 'c:\\\\Users\\\\adnan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\adnan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\adnan\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"]}],"source":["from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger') \n","import pandas as pd  \n","\n","# Define the data and labels\n","data = pd.read_csv('drug-dataset.csv')\n","labels = data['Active Ingredient'].tolist()\n","\n","#Preprocess the data \n","def preprocess_text(text):\n","    # Tokenize the text\n","    tokens = word_tokenize(text)\n","    # Convert tokens to lowercase\n","    tokens = [token.lower() for token in tokens]\n","    # Remove stop words\n","    stop_words = set(stopwords.words('english'))\n","    tokens = [token for token in tokens if token not in stop_words]\n","    # Join tokens back into a string\n","    preprocessed_text = ' '.join(tokens)\n","    return preprocessed_text  \n","\n","#train the model implementation\n","\n","def train_model(data, labels):\n","    # Create a count vectorizer\n","    vectorizer = CountVectorizer(preprocessor=preprocess_text)\n","    # Convert the text data into a bag-of-words representation\n","    data = vectorizer.fit_transform(data['Drug Name'].tolist())  # Use the 'Drug Name' column as data\n","    # Train a Naive Bayes classifier on the data\n","    classifier = MultinomialNB()\n","    classifier.fit(data, labels)\n","    return vectorizer, classifier   \n","\n","# Train the model\n","vectorizer, classifier = train_model(data, labels)\n","\n","# Prediction function\n","def predict_ingredients(text, vectorizer, classifier, threshold=0.2):\n","    # Preprocess the input text\n","    preprocessed_text = preprocess_text(text)\n","    # Convert the text into a bag-of-words representation\n","    data = vectorizer.transform([preprocessed_text])\n","    # Predict the probabilities of the classes\n","    probabilities = classifier.predict_proba(data)[0]\n","    print(\"Probabilities:\", probabilities)\n","    # Find the classes with probabilities above the threshold\n","    predicted_ingredients = []\n","    for i, probability in enumerate(probabilities):\n","        if probability > threshold:\n","            predicted_ingredients.append(classifier.classes_[i])\n","    # Check for incompatible mixtures\n","    INCOMPATIBLE_MIXTURES = [    \n","        ('Triamcinolone Acetonide  ', 'Fluvoxamine'),    \n","        ('Sildenafil', 'Nitrates'),    \n","        ('Simvastatin', 'Clarithromycin'),    \n","        ('Warfarin', 'Fluconazole'),    \n","        ('Methotrexate', 'Probenecid')\n","    ]\n","    if any(ingredient in predicted_ingredients for ingredient in INCOMPATIBLE_MIXTURES):\n","        print(\"Warning: Incompatible mixture detected!\")\n","    return predicted_ingredients  \n","\n","# Use the prediction function\n","text = \"This medicine contains Triazolam Halcion.\"\n","predicted_ingredients = predict_ingredients(text, vectorizer, classifier)\n","print(\"Predicted ingredients:\", predicted_ingredients)"]},{"cell_type":"code","execution_count":null,"id":"e038752e","metadata":{"id":"e038752e"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"}},"nbformat":4,"nbformat_minor":5}
